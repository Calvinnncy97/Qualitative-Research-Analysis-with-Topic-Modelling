{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import docx\n",
    "\n",
    "def preprocess_file (path): \n",
    "    files = glob.glob(\"C:\\\\Users\\\\ncy_k\\\\Desktop\\\\EMBRACE Redacted Transcripts\\\\Compiled Data\\\\\" + path)\n",
    "\n",
    "    para_list = []\n",
    "    tmp_list = []\n",
    "    include = False\n",
    "\n",
    "    for file in files:\n",
    "        paras = docx.Document(file).paragraphs\n",
    "        include = False\n",
    "        if paras != None:\n",
    "            for para in paras:\n",
    "                para = para.text\n",
    "                colon_index = para.find(\":\")\n",
    "                speaker = para[0:colon_index]\n",
    "                if colon_index != -1 and (speaker == \"LD\" or speaker == \"LD2\" or speaker == \"LD1\"):\n",
    "                    continue\n",
    "                para = para[colon_index+1:].strip().lower()\n",
    "                for word in para:\n",
    "                    if word.isdigit():\n",
    "                        para = para.replace(word,\"\")\n",
    "                if para.replace(\":\",\"\").isdigit():\n",
    "                    continue\n",
    "                if len(para)>0:\n",
    "                    para_list.append(para)\n",
    "    return para_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ncy_k\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ncy_k\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import gensim\n",
    "#measure topic coherence and opacity. Plot topic coherence measures with number of topics until it reaches max/plateau\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load(\"Google-News-Vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize and remove stop words by sentence\n",
    "def tokenize_remove_stopwords (para_list):\n",
    "    stopwords_list = [\"ok\", 'u', \"1\", \"one\", \"yea\", \"2\", \"u\", \"oh\", \"also\", \"yeah\", \"00\", \"okay\", \"\", \"hm\", \"haha\", \"5l\", \"mm\", \n",
    "                    \"k\", \"b\", \"le\", \"u\", \"lo\", \"p\", \"unintelligible\"]\n",
    "    #tokenize and remove stop words by sentence\n",
    "    all_sw = stopwords.words('English')\n",
    "    all_sw.extend(stopwords_list)\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokenized_and_cleaned_doc = []\n",
    "    print (f\"Length: {len(para_list)}\")\n",
    "    for i in para_list:\n",
    "        tokens = tokenizer.tokenize(i)\n",
    "        tokens_without_sw = []\n",
    "        tokens_without_sw = [wordnet_lemmatizer.lemmatize(word.strip()) for word in tokens if len(word) > 3 and not (word.strip() in all_sw)]\n",
    "        if len(tokens_without_sw) > 0: \n",
    "            tokenized_and_cleaned_doc.append(tokens_without_sw)\n",
    "\n",
    "    print(f\"{len(tokenized_and_cleaned_doc)} documents generated.\")\n",
    "    return tokenized_and_cleaned_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ncy_k\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ncy_k\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 516\n",
      "403 documents generated.\n",
      "Length: 271\n",
      "185 documents generated.\n",
      "Length: 89\n",
      "79 documents generated.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "filepaths = [\"Compiled Data Challenges.docx\", \"Compiled Data Challenges - Volunteer.docx\", \"Compiled Data Challenges - SAC Staff.docx\"]\n",
    "filenames = [\"Challenges-Full\", \"Challenges-Volunteers\", \"Challenges-SAC Staff\"]\n",
    "index = 0\n",
    "\n",
    "for path in filepaths:\n",
    "    para_list = preprocess_file(path)\n",
    "    tokenized_and_cleaned_doc = tokenize_remove_stopwords(para_list)\n",
    "    file = open(\"LFDLA-\" + filenames[index] +\".txt\", 'a', encoding=\"utf-8\")\n",
    "    for token in tokenized_and_cleaned_doc: \n",
    "        if len(token) > 0 and not (token is None):\n",
    "            file.write(\" \".join(token) + \"\\n\")\n",
    "    file.close()\n",
    "    index+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b96d62be92c1ea96eced9476d888f6c23b02c6ee1716ffab1285f05f13498421"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
