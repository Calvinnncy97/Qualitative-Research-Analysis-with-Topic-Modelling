{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_without_sw (file):\n",
    "    para_list = []\n",
    "    \n",
    "    paras = docx.Document(file).paragraphs\n",
    "    if paras != None:\n",
    "        for para in paras:\n",
    "            para = para.text\n",
    "            colon_index = para.find(\":\")\n",
    "            speaker = para[0:colon_index]\n",
    "            if colon_index != -1 and (speaker == \"LD\" or speaker == \"LD2\" or speaker == \"LD1\"):\n",
    "                continue\n",
    "            para = para[colon_index+1:].strip().lower()\n",
    "            for word in para:\n",
    "                if word.isdigit():\n",
    "                    para = para.replace(word,\"\")\n",
    "            if para.replace(\":\",\"\").isdigit():\n",
    "                continue\n",
    "            if len(para)>0:\n",
    "                para_list.append(para)\n",
    "        \n",
    "    #tokenize and remove stop words by sentence\n",
    "    all_sw = stopwords.words('English')\n",
    "    all_sw.extend(stopwords_list)\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokenized_and_cleaned_doc = []\n",
    "    print (f\"Length: {len(para_list)}\")\n",
    "    for i in para_list:\n",
    "        tokens = tokenizer.tokenize(i)\n",
    "        tokens_without_sw = []\n",
    "        tokens_without_sw = [wordnet_lemmatizer.lemmatize(word.strip()) for word in tokens]\n",
    "        if len(tokens_without_sw) > 0: \n",
    "            tokenized_and_cleaned_doc.append(tokens_without_sw)\n",
    "\n",
    "    print(f\"{len(tokenized_and_cleaned_doc)} documents generated.\")\n",
    "\n",
    "    return tokenized_and_cleaned_doc, len(tokenized_and_cleaned_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ncy_k\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ncy_k\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stopwords_list = [\"ok\", 'u', \"1\", \"one\", \"yea\", \"2\", \"u\", \"oh\", \"also\", \"yeah\", \"00\", \"okay\", \"\", \"hm\", \"haha\", \"5l\", \"mm\", \n",
    "                    \"k\", \"b\", \"le\", \"u\", \"lo\", \"p\", \"unintelligible\", \"actually\", \"subtitle\"]\n",
    "\n",
    "def preprocess (path):\n",
    "    print (f\"Path: {path}\")\n",
    "    files = glob.glob(path)\n",
    "\n",
    "    para_list = []\n",
    "\n",
    "    for file in files:\n",
    "        paras = docx.Document(file).paragraphs\n",
    "        if paras != None:\n",
    "            for para in paras:\n",
    "                para = para.text\n",
    "                colon_index = para.find(\":\")\n",
    "                speaker = para[0:colon_index]\n",
    "                if colon_index != -1 and (speaker == \"LD\" or speaker == \"LD2\" or speaker == \"LD1\"):\n",
    "                    continue\n",
    "                para = para[colon_index+1:].strip().lower()\n",
    "                for word in para:\n",
    "                    if word.isdigit():\n",
    "                        para = para.replace(word,\"\")\n",
    "                if para.replace(\":\",\"\").isdigit():\n",
    "                    continue\n",
    "                if len(para)>0:\n",
    "                    para_list.append(para)\n",
    "        \n",
    "    #tokenize and remove stop words by sentence\n",
    "    all_sw = stopwords.words('English')\n",
    "    all_sw.extend(stopwords_list)\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokenized_and_cleaned_doc = []\n",
    "    print (f\"Length: {len(para_list)}\")\n",
    "    for i in para_list:\n",
    "        tokens = tokenizer.tokenize(i)\n",
    "        tokens_without_sw = []\n",
    "        tokens_without_sw = [wordnet_lemmatizer.lemmatize(word.strip()) for word in tokens if len(word) > 3 and not (word.strip() in all_sw)]\n",
    "        if len(tokens_without_sw) > 0: \n",
    "            tokenized_and_cleaned_doc.append(tokens_without_sw)\n",
    "\n",
    "    print(f\"{len(tokenized_and_cleaned_doc)} documents generated.\")\n",
    "    \n",
    "    return tokenized_and_cleaned_doc, len(tokenized_and_cleaned_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 89\n",
      "86 documents generated.\n",
      "Path: C:\\Users\\ncy_k\\Desktop\\EMBRACE Redacted Transcripts\\Compiled Data\\Compiled Data Challenges - SAC Staff.docx\n",
      "Length: 89\n",
      "79 documents generated.\n",
      "Length: 271\n",
      "195 documents generated.\n",
      "Path: C:\\Users\\ncy_k\\Desktop\\EMBRACE Redacted Transcripts\\Compiled Data\\Compiled Data Challenges - Volunteer.docx\n",
      "Length: 271\n",
      "185 documents generated.\n",
      "Length: 516\n",
      "433 documents generated.\n",
      "Path: C:\\Users\\ncy_k\\Desktop\\EMBRACE Redacted Transcripts\\Compiled Data\\Compiled Data Challenges.docx\n",
      "Length: 516\n",
      "403 documents generated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Document Name</th>\n",
       "      <th>Raw/Processed</th>\n",
       "      <th>Number of Documents</th>\n",
       "      <th>Number of Tokens</th>\n",
       "      <th>Documents</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Compiled Data Challenges - SAC Staff.docx</td>\n",
       "      <td>Raw</td>\n",
       "      <td>86</td>\n",
       "      <td>4331</td>\n",
       "      <td>[[i, think, every, centre, ha, different, seni...</td>\n",
       "      <td>[i, think, every, centre, ha, different, senio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Compiled Data Challenges - SAC Staff.docx</td>\n",
       "      <td>Processed</td>\n",
       "      <td>79</td>\n",
       "      <td>1494</td>\n",
       "      <td>[[think, every, centre, different, senior, pro...</td>\n",
       "      <td>[think, every, centre, different, senior, prof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Compiled Data Challenges - Volunteer.docx</td>\n",
       "      <td>Raw</td>\n",
       "      <td>195</td>\n",
       "      <td>9887</td>\n",
       "      <td>[[i, don, t, think, so, since, most, of, what,...</td>\n",
       "      <td>[i, don, t, think, so, since, most, of, what, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Compiled Data Challenges - Volunteer.docx</td>\n",
       "      <td>Processed</td>\n",
       "      <td>185</td>\n",
       "      <td>3285</td>\n",
       "      <td>[[think, since, interest, doable], [pointing, ...</td>\n",
       "      <td>[think, since, interest, doable, pointing, tow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Compiled Data Challenges.docx</td>\n",
       "      <td>Raw</td>\n",
       "      <td>433</td>\n",
       "      <td>16385</td>\n",
       "      <td>[[i, think, every, centre, ha, different, seni...</td>\n",
       "      <td>[i, think, every, centre, ha, different, senio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Compiled Data Challenges.docx</td>\n",
       "      <td>Processed</td>\n",
       "      <td>403</td>\n",
       "      <td>5661</td>\n",
       "      <td>[[think, every, centre, different, senior, pro...</td>\n",
       "      <td>[think, every, centre, different, senior, prof...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Document Name                Raw/Processed Number of Documents Number of Tokens                     Documents                                             Text                       \n",
       "0  Compiled Data Challenges - SAC Staff.docx          Raw            86               4331       [[i, think, every, centre, ha, different, seni...  [i, think, every, centre, ha, different, senio...\n",
       "1  Compiled Data Challenges - SAC Staff.docx    Processed            79               1494       [[think, every, centre, different, senior, pro...  [think, every, centre, different, senior, prof...\n",
       "2  Compiled Data Challenges - Volunteer.docx          Raw           195               9887       [[i, don, t, think, so, since, most, of, what,...  [i, don, t, think, so, since, most, of, what, ...\n",
       "3  Compiled Data Challenges - Volunteer.docx    Processed           185               3285       [[think, since, interest, doable], [pointing, ...  [think, since, interest, doable, pointing, tow...\n",
       "4              Compiled Data Challenges.docx          Raw           433              16385       [[i, think, every, centre, ha, different, seni...  [i, think, every, centre, ha, different, senio...\n",
       "5              Compiled Data Challenges.docx    Processed           403               5661       [[think, every, centre, different, senior, pro...  [think, every, centre, different, senior, prof..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Data exploration:\n",
    "total word count before preprocessing\n",
    "total vocab before preprocessing\n",
    "total documents before preprocessing\n",
    "total vocab after preprocessing\n",
    "total tokens after preprocessing\n",
    "total documents after preprocessing\n",
    "\n",
    "Top 20 most frequent words\n",
    "Top 10 words with the highest TF-IDF scores\n",
    "\n",
    "Stop word list\n",
    "'''\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 10000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "df = pd.DataFrame(columns=[\"Document Name\", \"Raw/Processed\", \"Number of Documents\", \"Number of Tokens\", \"Documents\", \"Text\"]) \n",
    "dir = \"C:\\\\Users\\\\ncy_k\\\\Desktop\\\\EMBRACE Redacted Transcripts\\\\Compiled Data\\\\\"\n",
    "\n",
    "for file in glob.glob(\"C:\\\\Users\\\\ncy_k\\\\Desktop\\\\EMBRACE Redacted Transcripts\\\\Compiled Data\\\\*.docx\"):\n",
    "    raw_doc, raw_doc_num = preprocess_without_sw(file)\n",
    "    raw_text = [token for doc in raw_doc for token in doc]\n",
    "    proc_doc, proc_doc_num = preprocess(file)\n",
    "    proc_text = [token for doc in proc_doc for token in doc]\n",
    "    df = df.append({\"Document Name\": file.replace(dir, \"\"), \"Raw/Processed\": \"Raw\", \"Number of Documents\": raw_doc_num, \"Number of Tokens\": len(raw_text), \"Documents\":raw_doc, \"Text\": raw_text}, ignore_index=True)\n",
    "    df = df.append({\"Document Name\": file.replace(dir, \"\"), \"Raw/Processed\": \"Processed\", \"Number of Documents\": proc_doc_num, \"Number of Tokens\": len(proc_text), \"Documents\":proc_doc, \"Text\": proc_text}, ignore_index=True)\n",
    "    \n",
    "\n",
    "display (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_n_words(docs, n):\n",
    "    wordfreq = {}\n",
    "    for token in docs:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "\n",
    "    #get the words with the highest freq\n",
    "    import heapq\n",
    "    most_freq = heapq.nlargest(n, wordfreq, key=wordfreq.get)\n",
    "\n",
    "    return most_freq, len(wordfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def top_tfidf_n_words (docs, n):\n",
    "    tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "    tfIdf = tfIdfVectorizer.fit_transform(\" \". join(i) for i in docs)\n",
    "    df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "    df = df.sort_values('TF-IDF', ascending=False)\n",
    "\n",
    "    return df.head(n).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_num = []\n",
    "top_freq = []\n",
    "top_tfidf = []\n",
    "\n",
    "n_freq = 20\n",
    "n_tfidf = 10\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    top_n_words, vocab = most_frequent_n_words(row[\"Text\"], n_freq)\n",
    "    vocab_num.append(vocab)\n",
    "    top_freq.append(top_n_words)\n",
    "    top_tfidf.append(top_tfidf_n_words(row[\"Documents\"], n_tfidf))\n",
    "\n",
    "df[\"Most Frequent 20 Words\"] = top_freq\n",
    "df[\"Top 10 TF-IDF Words\"] = top_tfidf\n",
    "df[\"Vocab Length\"] = vocab_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Document Name</th>\n",
       "      <th>Raw/Processed</th>\n",
       "      <th>Number of Documents</th>\n",
       "      <th>Number of Tokens</th>\n",
       "      <th>Documents</th>\n",
       "      <th>Text</th>\n",
       "      <th>Most Frequent 20 Words</th>\n",
       "      <th>Top 10 TF-IDF Words</th>\n",
       "      <th>Vocab Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Compiled Data Challenges - SAC Staff.docx</td>\n",
       "      <td>Raw</td>\n",
       "      <td>86</td>\n",
       "      <td>4331</td>\n",
       "      <td>[[i, think, every, centre, ha, different, seni...</td>\n",
       "      <td>[i, think, every, centre, ha, different, senio...</td>\n",
       "      <td>[the, they, to, we, and, a, are, so, of, that,...</td>\n",
       "      <td>{'TF-IDF': {'every': 0.48240699512568963, 'ha'...</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Compiled Data Challenges - SAC Staff.docx</td>\n",
       "      <td>Processed</td>\n",
       "      <td>79</td>\n",
       "      <td>1494</td>\n",
       "      <td>[[think, every, centre, different, senior, pro...</td>\n",
       "      <td>[think, every, centre, different, senior, prof...</td>\n",
       "      <td>[senior, know, come, like, centre, time, exerc...</td>\n",
       "      <td>{'TF-IDF': {'every': 0.5387983337953228, 'diff...</td>\n",
       "      <td>558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Compiled Data Challenges - Volunteer.docx</td>\n",
       "      <td>Raw</td>\n",
       "      <td>195</td>\n",
       "      <td>9887</td>\n",
       "      <td>[[i, don, t, think, so, since, most, of, what,...</td>\n",
       "      <td>[i, don, t, think, so, since, most, of, what, ...</td>\n",
       "      <td>[to, the, they, i, a, that, them, and, it, you...</td>\n",
       "      <td>{'TF-IDF': {'doable': 0.3744346150850208, 'sin...</td>\n",
       "      <td>1203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Compiled Data Challenges - Volunteer.docx</td>\n",
       "      <td>Processed</td>\n",
       "      <td>185</td>\n",
       "      <td>3285</td>\n",
       "      <td>[[think, since, interest, doable], [pointing, ...</td>\n",
       "      <td>[think, since, interest, doable, pointing, tow...</td>\n",
       "      <td>[senior, know, sometimes, like, come, phone, t...</td>\n",
       "      <td>{'TF-IDF': {'doable': 0.5905620872930636, 'sin...</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Compiled Data Challenges.docx</td>\n",
       "      <td>Raw</td>\n",
       "      <td>433</td>\n",
       "      <td>16385</td>\n",
       "      <td>[[i, think, every, centre, ha, different, seni...</td>\n",
       "      <td>[i, think, every, centre, ha, different, senio...</td>\n",
       "      <td>[to, the, they, i, you, a, we, and, it, of, th...</td>\n",
       "      <td>{'TF-IDF': {'profile': 0.45661658324704896, 'e...</td>\n",
       "      <td>1612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Compiled Data Challenges.docx</td>\n",
       "      <td>Processed</td>\n",
       "      <td>403</td>\n",
       "      <td>5661</td>\n",
       "      <td>[[think, every, centre, different, senior, pro...</td>\n",
       "      <td>[think, every, centre, different, senior, prof...</td>\n",
       "      <td>[know, senior, think, thing, like, come, somet...</td>\n",
       "      <td>{'TF-IDF': {'profile': 0.49978429801143837, 'e...</td>\n",
       "      <td>1356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Document Name                Raw/Processed Number of Documents Number of Tokens                     Documents                                             Text                                      Most Frequent 20 Words                              Top 10 TF-IDF Words                  Vocab Length\n",
       "0  Compiled Data Challenges - SAC Staff.docx          Raw            86               4331       [[i, think, every, centre, ha, different, seni...  [i, think, every, centre, ha, different, senio...  [the, they, to, we, and, a, are, so, of, that,...  {'TF-IDF': {'every': 0.48240699512568963, 'ha'...       737    \n",
       "1  Compiled Data Challenges - SAC Staff.docx    Processed            79               1494       [[think, every, centre, different, senior, pro...  [think, every, centre, different, senior, prof...  [senior, know, come, like, centre, time, exerc...  {'TF-IDF': {'every': 0.5387983337953228, 'diff...       558    \n",
       "2  Compiled Data Challenges - Volunteer.docx          Raw           195               9887       [[i, don, t, think, so, since, most, of, what,...  [i, don, t, think, so, since, most, of, what, ...  [to, the, they, i, a, that, them, and, it, you...  {'TF-IDF': {'doable': 0.3744346150850208, 'sin...      1203    \n",
       "3  Compiled Data Challenges - Volunteer.docx    Processed           185               3285       [[think, since, interest, doable], [pointing, ...  [think, since, interest, doable, pointing, tow...  [senior, know, sometimes, like, come, phone, t...  {'TF-IDF': {'doable': 0.5905620872930636, 'sin...       986    \n",
       "4              Compiled Data Challenges.docx          Raw           433              16385       [[i, think, every, centre, ha, different, seni...  [i, think, every, centre, ha, different, senio...  [to, the, they, i, you, a, we, and, it, of, th...  {'TF-IDF': {'profile': 0.45661658324704896, 'e...      1612    \n",
       "5              Compiled Data Challenges.docx    Processed           403               5661       [[think, every, centre, different, senior, pro...  [think, every, centre, different, senior, prof...  [know, senior, think, thing, like, come, somet...  {'TF-IDF': {'profile': 0.49978429801143837, 'e...      1356    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)\n",
    "df.to_csv(\"Dataset Exploration Features\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df7d3086dab6f8310a40586db14da3861c7874955cf9954b6e02c08d11a1e38f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
